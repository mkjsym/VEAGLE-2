# python eagle/ge_data/ge_data_all_llava_unified_topk.py --start 0 --end 67999 --outdir /data/dataset_cls_top100 --auto_distribute
import argparse
import os
import sys
import subprocess
import math
import time

# -----------------------------------------------------------------------------
# 1. Argument Parsing (Launcher & Worker ê³µí†µ)
# -----------------------------------------------------------------------------
parser = argparse.ArgumentParser(description='AirCache Data Generation')
parser.add_argument('--start', type=int, default=0, help='Total start index (or sub-start in worker mode)')
parser.add_argument('--end', type=int, default=100, help='Total end index (or sub-end in worker mode)')
parser.add_argument('--index', type=int, default=0, help='Sub-directory index for output')
parser.add_argument('--gpu_index', type=int, nargs='+', default=[0], help='Specific GPU ID to use (worker mode)')
parser.add_argument('--outdir', type=str, default='outdir0', help='Output directory')
parser.add_argument('--auto_distribute', action='store_true', help='[Launcher Mode] Automatically split workload across all available GPUs')

args = parser.parse_args()

# -----------------------------------------------------------------------------
# 2. Launcher Logic (ìë™ ë¶„ì‚° ì²˜ë¦¬)
# -----------------------------------------------------------------------------
if args.auto_distribute:
    import torch
    
    # ê°€ìš© GPU ê°œìˆ˜ í™•ì¸
    num_gpus = torch.cuda.device_count()
    if num_gpus == 0:
        print("âŒ No GPUs found. Exiting.")
        sys.exit(1)
        
    print(f"ğŸš€ [Launcher] Found {num_gpus} GPUs. Distributing workload...")

    total_samples = args.end - args.start
    chunk_size = math.ceil(total_samples / num_gpus)
    
    processes = []
    
    for rank in range(num_gpus):
        # ê° GPUê°€ ë‹´ë‹¹í•  ë°ì´í„° ë²”ìœ„ ê³„ì‚°
        sub_start = args.start + (rank * chunk_size)
        sub_end = min(args.start + ((rank + 1) * chunk_size), args.end)
        
        # ë²”ìœ„ê°€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ìŠ¤í‚µ (ë°ì´í„°ê°€ GPU ìˆ˜ë³´ë‹¤ ì ì„ ë•Œ)
        if sub_start >= sub_end:
            break

        # Worker ì‹¤í–‰ ëª…ë ¹ì–´ êµ¬ì„±
        # ìê¸° ìì‹ (__file__)ì„ í˜¸ì¶œí•˜ë˜, auto_distribute ì˜µì…˜ì„ ë¹¼ê³  ì‹¤í–‰
        cmd = [
            sys.executable, __file__,
            '--start', str(sub_start),
            '--end', str(sub_end),
            '--index', str(rank),        # ê° GPUë³„ë¡œ ë‹¤ë¥¸ í´ë”(0, 1, 2...)ì— ì €ì¥í•˜ì—¬ ì¶©ëŒ ë°©ì§€
            '--gpu_index', str(rank),    # ê° í”„ë¡œì„¸ìŠ¤ëŠ” í•´ë‹¹ rankì˜ GPU 1ê°œë§Œ í• ë‹¹ë°›ìŒ
            '--outdir', args.outdir
        ]
        
        print(f"   [GPU {rank}] Processing indices {sub_start} ~ {sub_end} -> Saving to {args.outdir}/{rank}")
        
        # ë¹„ë™ê¸° ì‹¤í–‰ (subprocess)
        proc = subprocess.Popen(cmd)
        processes.append(proc)

    # ëª¨ë“  ì‘ì—…ì´ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
    exit_codes = [p.wait() for p in processes]
    
    if all(code == 0 for code in exit_codes):
        print(f"âœ… [Launcher] All {len(processes)} jobs completed successfully.")
    else:
        print(f"âš ï¸ [Launcher] Some jobs failed. Exit codes: {exit_codes}")
        
    sys.exit(0)

# -----------------------------------------------------------------------------
# 3. Worker Logic (ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬)
# -----------------------------------------------------------------------------

# [ì¤‘ìš”] CUDA_VISIBLE_DEVICES ì„¤ì •ì€ torch import ì „ì— í•´ì•¼ í•¨
# Worker ëª¨ë“œì—ì„œëŠ” gpu_indexê°€ 1ê°œë§Œ ë“¤ì–´ì˜¨ë‹¤ê³  ê°€ì •
os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu_index[0])

import gc
import copy
import torch
import torch.nn.functional as F
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor, BitsAndBytesConfig, LlavaForConditionalGeneration
from datasets import load_dataset, concatenate_datasets
import json
from fastchat.model.model_adapter import get_conversation_template
from PIL import Image

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
bigname = "/data/youngmin/models/llava-1.5-7b-hf"

print(f"ğŸ”§ [Worker GPU {args.gpu_index[0]}] Initializing... Range: {args.start}-{args.end}")

def keep_topk_image_token(
    input_ids,
    loss_mask,
    hidden_states,
    image_features,
    attentions,
    img_tok_index=32000,
    topk=100,
):
    """
    input_ids: [1, seq_len]
    loss_mask: [1, seq_len]
    hidden_states: [1, seq_len, dim]
    attentions: list of [1, heads, seq_len, seq_len]
    image_features: [1, 576, dim] or [576, dim]
    """
    device = input_ids.device

    # ì°¨ì› ì¶•ì†Œ
    input_ids = input_ids[0].to(device)        # [seq_len]
    loss_mask = loss_mask[0].to(device)        # [seq_len]
    hidden_states = hidden_states[0].to(device)  # [seq_len, dim]
    
    # CLS í† í° ì¸ë±ìŠ¤ ì°¾ê¸°
    cls_positions = (input_ids == img_tok_index).nonzero(as_tuple=True)[0]
    if cls_positions.numel() == 0:
        # CLS í† í° ì—†ìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ
        return input_ids.unsqueeze(0), loss_mask.unsqueeze(0), hidden_states, image_features
    cls_index = cls_positions[0].item()

    # ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ CLSì–´í…ì…˜ ì ìˆ˜
    last_layer_attn = attentions[-1][0].to(device)  # [heads, seq_len, seq_len]
    # CLS í† í°ì´ attendí•œ ê° í† í°ë³„ í‰ê·  ì ìˆ˜
    attn_scores = last_layer_attn[:, cls_index, :].mean(dim=0)  # [seq_len]

    # ëª¨ë“  ì´ë¯¸ì§€ í† í° ìœ„ì¹˜
    image_token_indices = (input_ids == img_tok_index).nonzero(as_tuple=True)[0]

    # top-k ì´ë¯¸ì§€ í† í° ë½‘ê¸°
    scores = attn_scores[image_token_indices].float()
    k = min(topk, scores.size(0))
    topk_local_idxs = torch.topk(scores, k).indices  # local indices
    topk_global_idxs = image_token_indices[topk_local_idxs]

    # CLS ì¸ë±ìŠ¤ë„ ì¶”ê°€
    topk_global = torch.cat([topk_global_idxs, torch.tensor([cls_index], device=device)])
    topk_global = torch.unique(topk_global)

    # í•„í„° ë§ˆìŠ¤í¬
    text_mask = input_ids != img_tok_index
    img_mask = torch.zeros_like(input_ids, dtype=torch.bool, device=device)
    img_mask[topk_global] = True
    final_mask = text_mask | img_mask

    # í•„í„°ë§
    filtered_input_ids = input_ids[final_mask].unsqueeze(0)
    filtered_loss_mask = loss_mask[final_mask].unsqueeze(0)
    filtered_hidden_states = hidden_states[final_mask]

    # ì´ë¯¸ì§€ í”¼ì²˜ í•„í„°ë§
    filtered_image_features = None
    if image_features is not None:
        feat = image_features[0] if image_features.dim() == 3 else image_features
        feat = feat.to(device)
        # topk_local_idxsëŠ” CLS ì œì™¸í•œ ìˆœìˆ˜ ì´ë¯¸ì§€ í† í°ì´ë¯€ë¡œ,
        # ì‹¤ì œ í”¼ì²˜ì—ì„œ ë½‘ì„ ë•ŒëŠ” local_idxsë§Œ ì‚¬ìš©
        filtered_image_features = feat[topk_local_idxs]

    return filtered_input_ids, filtered_loss_mask, filtered_hidden_states, filtered_image_features

# -----------------------------------------------------------------------------
# AirCache Function Implementation
# -----------------------------------------------------------------------------
def keep_topk_image_token_aircache(
    input_ids,
    loss_mask,
    hidden_states,
    image_features,
    attentions,
    img_tok_index=32000,
    topk=100,
    alpha=0.9
):
    """
    AirCache: Activating Inter-modal Relevancy KV Cache Compression êµ¬í˜„
    [cite_start]Elite Observation Window ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ í† í°ì„ í•„í„°ë§í•©ë‹ˆë‹¤. [cite: 239]
    """
    device = input_ids.device

    input_ids = input_ids[0].to(device)
    loss_mask = loss_mask[0].to(device)
    hidden_states = hidden_states[0].to(device)
    
    # ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ Attention Map ê°€ì ¸ì˜¤ê¸° (Heads í‰ê· )
    last_layer_attn = attentions[-1][0].mean(dim=0).to(device) 

    # 1. í…ìŠ¤íŠ¸ í† í°ê³¼ ì´ë¯¸ì§€ í† í° ì¸ë±ìŠ¤ êµ¬ë¶„
    is_img_token = (input_ids == img_tok_index)
    img_indices = is_img_token.nonzero(as_tuple=True)[0]
    text_indices = (~is_img_token).nonzero(as_tuple=True)[0]

    if img_indices.numel() == 0:
        return input_ids.unsqueeze(0), loss_mask.unsqueeze(0), hidden_states, image_features

    # [cite_start]2. Key Text Token Selection (Elite Observation Window êµ¬ì„±) [cite: 165]
    anchor_idx = input_ids.size(0) - 1 
    text_attn_scores = last_layer_attn[anchor_idx, text_indices] 

    # [cite_start]Eq (5): Relevance Threshold(alpha)ë¥¼ ì´ìš©í•œ í•„í„°ë§ [cite: 167]
    max_text_score = text_attn_scores.max()
    threshold_score = alpha * max_text_score
    
    key_text_mask = text_attn_scores >= threshold_score
    key_text_indices = text_indices[key_text_mask]

    # [cite_start]3. Visual Token Importance Assessment [cite: 178-179]
    attn_from_key_text = last_layer_attn[key_text_indices, :][:, img_indices]
    visual_scores = attn_from_key_text.mean(dim=0)

    # 4. Top-K ì´ë¯¸ì§€ í† í° ì„ ì • ë° í•„í„°ë§
    k = min(topk, visual_scores.size(0))
    topk_local_idxs = torch.topk(visual_scores, k).indices
    
    topk_global_idxs = img_indices[topk_local_idxs]
    topk_global_idxs, _ = torch.sort(topk_global_idxs)

    # 5. ë§ˆìŠ¤í¬ ìƒì„± ë° ë°ì´í„° ì¬êµ¬ì„±
    keep_mask = torch.zeros_like(input_ids, dtype=torch.bool)
    keep_mask[text_indices] = True
    keep_mask[topk_global_idxs] = True

    filtered_input_ids = input_ids[keep_mask].unsqueeze(0)
    filtered_loss_mask = loss_mask[keep_mask].unsqueeze(0)
    filtered_hidden_states = hidden_states[keep_mask]

    filtered_image_features = None
    if image_features is not None:
        feat = image_features[0] if image_features.dim() == 3 else image_features
        feat = feat.to(device)
        sorted_local_idxs, _ = torch.sort(topk_local_idxs)
        filtered_image_features = feat[sorted_local_idxs]

    return filtered_input_ids, filtered_loss_mask, filtered_hidden_states, filtered_image_features

# -----------------------------------------------------------------------------
# Dataset & Model Loading
# -----------------------------------------------------------------------------
def build_dataset_rank(tokenizer, split="train", select=None):
    # Processor ê²½ë¡œ ìˆ˜ì • (bignameê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •)
    processor = AutoProcessor.from_pretrained(bigname)
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ (ì‚¬ìš©ì í™˜ê²½ì— ë§ì¶¤)
    ds1 = load_dataset('json', data_files="/data/llava_instruct_150k.json")[split]
    # ds1 ì´ë¯¸ì§€ ê²½ë¡œ: COCO
    ds1 = ds1.add_column('image_folder', ['/data/coco/train2017'] * len(ds1))
    
    ds2 = load_dataset('json', data_files="/data/sharegpt4v_instruct_gpt4-vision_cap100k.json")[split]
    # ds2 ì´ë¯¸ì§€ ê²½ë¡œ: COCO
    ds2 = ds2.add_column('image_folder', ['/data'] * len(ds2))
    
    # ë³‘í•© ë° ì„ íƒ
    ds = concatenate_datasets([ds1, ds2]).shuffle(seed=41)
    
    # [Worker] í• ë‹¹ëœ ë²”ìœ„ë§Œ ì„ íƒ
    ds = ds.select(range(args.start, args.end))
        
    original_columns = ds.column_names
    num_proc = 4
    
    def contains_special_token(turn, tokenizer, special_token_id=32000):
        input_ids = tokenizer(turn).input_ids
        return special_token_id in input_ids

    def preprocess_function(examples):
        new_examples = {
            "conversation":[], "input_ids": [], "image": [], "pixel_values":[], "loss_mask": []
        }
        for i in range(len(examples['id'])):
            conv = get_conversation_template("vicuna")
            roles = {"human": conv.roles[0], "gpt": conv.roles[1]}
            sorce= examples['conversations'][i]
            
            if roles[sorce[0]["from"]] != conv.roles[0]:
                sorce = sorce[1:]
            conv.messages = []
            for j, sentence in enumerate(sorce):
                role = roles[sentence["from"]]
                assert role == conv.roles[j % 2], f"{i}"
                conv.append_message(role, sentence["value"])
            conversation=conv.get_prompt()
            
            image_file = examples['image'][i]
            folder = examples['image_folder'][i]
            try:
                image = Image.open(os.path.join(folder, image_file)).convert('RGB')
                inputs = processor(images=image, text=conversation, return_tensors="pt")
                input_ids=torch.as_tensor(inputs["input_ids"])[0]
                pixel_values=torch.as_tensor(inputs["pixel_values"])[0]
                loss_mask=torch.ones_like(input_ids)
                
                sep = conv.sep + conv.roles[1] + ": "
                turns = conversation.split(conv.sep2)
                
                cur_len = 1
                loss_mask[:cur_len] = 0
                for i, turn in enumerate(turns):
                    if turn == "": break
                    is_im_token = contains_special_token(turn,tokenizer)
                    turn_len = len(tokenizer(turn).input_ids)
                    if is_im_token : turn_len+=576

                    parts = turn.split(sep)
                    if len(parts) != 2: break
                    parts[0] += sep
                    instruction_len = len(tokenizer(parts[0]).input_ids) - 2
                    if is_im_token : instruction_len+=576
                    
                    if i==0: instruction_len -= 1
                    
                    loss_mask[cur_len: cur_len + instruction_len] = 0
                    cur_len += turn_len
                    if i==0: cur_len -= 1
                loss_mask[cur_len:] = 0
                
                new_examples["conversation"].append(conversation)
                new_examples["input_ids"].append(input_ids[None,:])
                new_examples["image"].append(image_file)
                new_examples["pixel_values"].append(pixel_values[None,:])
                new_examples["loss_mask"].append(loss_mask[None,:])
            except Exception as e:
                print(f"Skipping {image_file} due to error: {e}")
                continue

        return new_examples

    ds = ds.map(
        preprocess_function,
        batched=True,
        num_proc=num_proc,
        remove_columns=original_columns,
        load_from_cache_file=False
    )

    ds.set_format(type="torch")
    return ds

# Tokenizer & Model Setup
# tokenizer ê²½ë¡œ ìˆ˜ì • (bigname ì‚¬ìš©)
bigtokenizer = AutoProcessor.from_pretrained(bigname).tokenizer
ds = build_dataset_rank(bigtokenizer)
print(f"[Worker GPU {args.gpu_index[0]}] Dataset loaded. Size: {len(ds)}")

# [ìˆ˜ì • ì „]
bigmodel = LlavaForConditionalGeneration.from_pretrained(bigname, device_map="cuda", torch_dtype=torch.float16, attn_implementation="eager")

# [ìˆ˜ì • í›„] 8-bit Quantization ì ìš©
# bnb_config = BitsAndBytesConfig(
#     load_in_8bit=True,
#     llm_int8_skip_modules=["mm_projector", "vision_tower"]  # ë¹„ì „ ê´€ë ¨ ëª¨ë“ˆì€ ì •ë°€ë„ ìœ ì§€ë¥¼ ìœ„í•´ fp16 ìœ ì§€ ê¶Œì¥
# )

# bigmodel = LlavaForConditionalGeneration.from_pretrained(
#     bigname, 
#     device_map="cuda", 
#     quantization_config=bnb_config, # 8ë¹„íŠ¸ ì„¤ì • ì ìš©
#     # torch_dtype=torch.float16,    # 8bit ë¡œë“œì‹œì—ëŠ” ë³´í†µ ìë™ ì²˜ë¦¬ë˜ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ë†”ë‘¬ë„ ë¬´ë°©
#     attn_implementation="eager"     # output_attentions=Trueë¥¼ ìœ„í•´ eager ìœ ì§€
# )

bigmodel.eval()

# -----------------------------------------------------------------------------
# Main Generation Loop
# -----------------------------------------------------------------------------
@torch.no_grad()
def ge(data):
    input_ids = data["input_ids"]
    pixel_values = data["pixel_values"]
    loss_mask = data["loss_mask"]
    
    outs_big = bigmodel(input_ids.cuda(), pixel_values.cuda(), output_hidden_states=True, output_attentions=True)
    
    image_features = outs_big.image_hidden_states.cpu()
    hidden_state_big = outs_big.hidden_states[-1].cpu()
    
    # [AirCache Algorithm Applied]
    input_ids, loss_mask, hidden_state_big, image_features = keep_topk_image_token(
        input_ids, 
        loss_mask, 
        hidden_state_big, 
        image_features, 
        outs_big.attentions,
        img_tok_index=32000,
        topk=100
    )
    
    del outs_big
    gc.collect()
    torch.cuda.empty_cache()
    
    td = {
        "input_ids": input_ids.cpu()[0],
        "image": data["image"],
        "hidden_state": hidden_state_big.cpu(),
        "loss_mask": loss_mask.cpu()[0], 
        "image_features": image_features.cpu()
    }
    
    del hidden_state_big
    gc.collect()
    torch.cuda.empty_cache()
    
    return td

# Output directory setup
outdir_sub = f'{args.outdir}/{args.index}'
if not os.path.exists(outdir_sub):
    try:
        os.makedirs(outdir_sub)
    except FileExistsError:
        pass

def writedata(name, data_point):
    if not os.path.exists(name):
        os.makedirs(name)
    current_length = len(os.listdir(name))
    idx = current_length
    torch.save(data_point, f'{name}/data_{idx}.ckpt')

# Processing Loop ë‚´ë¶€ ìˆ˜ì •
for data in tqdm(ds, desc=f"GPU {args.gpu_index[0]}"):
    # [ì¶”ê°€] ì‹œí€€ìŠ¤ ê¸¸ì´ ì²´í¬ (ì˜ˆ: 3500 í† í° ì´ìƒì´ë©´ ìŠ¤í‚µ)
    # 24GB ë©”ëª¨ë¦¬ì—ì„œ output_attentions=Trueì¼ ë•Œ 4096 í’€ ì‹œí€€ìŠ¤ëŠ” í„°ì§ˆ ìˆ˜ ìˆìŒ
    seq_len = data["input_ids"].shape[1]
    if seq_len > 3500:
        print(f"âš ï¸ Skipping data due to length ({seq_len} tokens) to avoid OOM.")
        continue

    try:
        with torch.no_grad():
            outdata = ge(data)
        
        writedata(outdir_sub, outdata)
        
        del outdata
    except torch.cuda.OutOfMemoryError:
        print(f"âŒ OOM Error encountered at length {seq_len}. Skipping this sample.")
        torch.cuda.empty_cache() # ë©”ëª¨ë¦¬ ë¹„ìš°ê³  ë‹¤ìŒìœ¼ë¡œ ì§„í–‰
        continue
    except Exception as e:
        print(f"âš ï¸ Error processing data: {e}")
        continue
    
    gc.collect()
    torch.cuda.empty_cache()

print(f"âœ… [Worker GPU {args.gpu_index[0]}] Finished processing.")
